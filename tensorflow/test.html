<!DOCTYPE html>
<html lang="en">
<head>
  <title>al_test</title>

</head>
<body>
<video id="webcam" autoplay muted width="640" height="480"></video>


<canvas id="myCanvas" width="640" height="480"></canvas>



<!-- Import TensorFlow.js library -->

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
<!-- Import the page's JavaScript to do some stuff -->
<!--  <script src="script.js" defer></script> -->
<script>
		const video = document.getElementById('webcam');
		const myCanvas = document.getElementById('myCanvas');

		// Check if webcam access is supported.
		function getUserMediaSupported() {
		  return !!(navigator.mediaDevices &&
			navigator.mediaDevices.getUserMedia);
		}

		// If webcam supported, add event listener to button for when user
		// wants to activate it to call enableCam function which we will
		// define in the next step.
		if (getUserMediaSupported()) {
		  enableCam()
		} else {
		  console.warn('getUserMedia() is not supported by your browser');
		}

		// Placeholder function for next step. Paste over this in the next step.
		// Enable the live webcam view and start classification.
		function enableCam() {
		  // getUsermedia parameters to force video but not audio.
		  const constraints = {
			video: true
		  };
		  // Activate the webcam stream.
		  navigator.mediaDevices.getUserMedia(constraints).then(function(stream) {
			video.srcObject = stream;
			video.addEventListener('loadeddata', main);
		  });
		}

		// Placeholder function for next step.
		//function predictWebcam() {
			//console.log("al_test");
			//requestAnimationFrame(()=>{predictWebcam()});
		//}
		
		let start_time = new Date().getTime();
		
		const getFrameFromVideo = (video, canvas , a ,b ,c, d) => {
			let now_time = new Date().getTime();
			if (now_time-start_time){
				start_time = now_time;
				const ctx = canvas.getContext("2d");
				ctx.clearRect(0, 0, canvas.width, canvas.height);
				ctx.save();   //儲存狀態
				ctx.translate(video.width, 0);
				ctx.scale(-1, 1);
				ctx.drawImage(video, 0, 0, video.width, video.height);
				ctx.fillRect(a, b, c, d);
				ctx.restore(); //到此才輸出，才不會還沒整體操作完就放出，會造成畫面快速抖動
			}
			//requestAnimationFrame(() => getFrameFromVideo(video, canvas));
			
		};
				
		
		//================================================================================
		const model;
		
		async function main() {
		  // Load the model.
		  if (!model){
			model = await blazeface.load()
		  }
		  // Pass in an image or video to the model. The model returns an array of
		  // bounding boxes, probabilities, and landmarks, one for each detected face.

		  const returnTensors = false; // Pass in `true` to get tensors back, rather than values.
		  const predictions = await model.estimateFaces(video, returnTensors);

		  if (predictions.length > 0) {
			/*
			`predictions` is an array of objects describing each detected face, for example:

			[
			  {
				topLeft: [232.28, 145.26],
				bottomRight: [449.75, 308.36],
				probability: [0.998],
				landmarks: [
				  [295.13, 177.64], // right eye
				  [382.32, 175.56], // left eye
				  [341.18, 205.03], // nose
				  [345.12, 250.61], // mouth
				  [252.76, 211.37], // right ear
				  [431.20, 204.93] // left ear
				]
			  }
			]
			*/

			for (let i = 0; i < predictions.length; i++) {
			  const start = predictions[i].topLeft;
			  const end = predictions[i].bottomRight;
			  const size = [end[0] - start[0], end[1] - start[1]];

			  // Render a rectangle over each detected face.
			  getFrameFromVideo(video,myCanvas,start[0], start[1], size[0], size[1]);
			}
		  }
		}


	</script>
</body>
</html>
