<html>
<head>
<!-- Require the peer dependencies of face-landmarks-detection. -->
<script src="https://unpkg.com/@tensorflow/tfjs-core@2.4.0/dist/tf-core.js"></script>
<script src="https://unpkg.com/@tensorflow/tfjs-converter@2.4.0/dist/tf-converter.js"></script>

<!-- You must explicitly require a TF.js backend if you're not using the tfjs union bundle. -->
<script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@2.4.0/dist/tf-backend-webgl.js"></script>
<!-- Alternatively you can use the WASM backend: <script src="https://unpkg.com/@tensorflow/tfjs-backend-wasm@2.4.0/dist/tf-backend-wasm.js"></script> -->

<!-- Require face-landmarks-detection itself. -->
<script src="https://unpkg.com/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
</head>
<body>

<!--
<video width="320" height="240" crossorigin="anonymous" autoplay muted>
  <source src="https://media.istockphoto.com/videos/male-hand-gestures-green-screen-and-alpha-matte-video-id483074107" type="video/mp4">
Your browser does not support the video tag.
</video>

-->

<!--
<img width="300" height="300" id="image_test" src="https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/fotojet-6-1595383950.jpg?crop=0.497xw:0.993xh;0,0&resize=640:*" crossorigin="anonymous">
-->
<!--
<img width="300" height="300" id="image_test" src="https://img.4gamers.com.tw/puku-clone-version/a6458762b0c4f0f5dced9e05d6bc94afe719a02f.jpg" crossorigin="anonymous">
-->
<img width="225" height="224" id="image_test" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMnIS0DtiUcoZq0_pnzI_B-NKVHSElZkOpkg&usqp=CAU" crossorigin="anonymous">

<canvas width="225" height="224" id="test"></canvas>


<script
  src="https://code.jquery.com/jquery-3.6.0.slim.min.js"
  integrity="sha256-u7e5khyithlIdTpu22PHhENmPcRdFiHRjhAuHcs05RI="
  crossorigin="anonymous"></script>
<script>

async function main() {
    const model = await faceLandmarksDetection.load(
    faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);
	const image = document.querySelector('#image_test');
	console.log("image width: " + image.width);
	console.log("image height: " + image.height);
  // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain an
  // array of detected faces from the MediaPipe graph. If passing in a video
  // stream, a single prediction per frame will be returned.
  const predictions = await model.estimateFaces({
    input: document.querySelector("#image_test")
  });
    console.log("al_test : " + predictions.length);
	
	
	console.log("al_test predictions : " + JSON.stringify(predictions));

  if (predictions.length > 0) {
    /*
    `predictions` is an array of objects describing each detected face, for example:

    [
      {
        faceInViewConfidence: 1, // The probability of a face being present.
        boundingBox: { // The bounding box surrounding the face.
          topLeft: [232.28, 145.26],
          bottomRight: [449.75, 308.36],
        },
        mesh: [ // The 3D coordinates of each facial landmark.
          [92.07, 119.49, -17.54],
          [91.97, 102.52, -30.54],
          ...
        ],
        scaledMesh: [ // The 3D coordinates of each facial landmark, normalized.
          [322.32, 297.58, -17.54],
          [322.18, 263.95, -30.54]
        ],
        annotations: { // Semantic groupings of the `scaledMesh` coordinates.
          silhouette: [
            [326.19, 124.72, -3.82],
            [351.06, 126.30, -3.00],
            ...
          ],
          ...
        }
      }
    ]
    */

    for (let i = 0; i < predictions.length; i++) {
      const keypoints = predictions[i].scaledMesh;

      // Log facial keypoints.
      for (let i = 0; i < keypoints.length; i++) {
        const [x, y, z] = keypoints[i];

        console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);
      }
	  
	  
	  const boundingBox = predictions[i].boundingBox;
	  console.log("topLeft:" + boundingBox.topLeft);
	  console.log("bottomRight:" + boundingBox.bottomRight);
	  let topLeft = boundingBox.topLeft;
	  let bottomRight = boundingBox.bottomRight;
	  
	  //myCrop = $("#image_test").croppie({
		
	  //})
	  const canvas = document.querySelector('#test');
		const context = canvas.getContext('2d');
		context.drawImage(image, 0, 0);
		context.moveTo(topLeft[0], 0);
		context.lineTo(topLeft[0], 224);
		context.moveTo(bottomRight[0], 0);
		context.lineTo(bottomRight[0], 224);
		context.strokeStyle = '#00ff00';
		context.stroke();
		
		context.moveTo(0, topLeft[1]);
		context.lineTo(225, topLeft[1]);
		context.moveTo(0, bottomRight[1]);
		context.lineTo(225, bottomRight[1]);
		context.strokeStyle = '#ff0000';
		context.stroke();
		//context.drawImage(image, topLeft[1], topLeft[0]);
		//context.drawImage(image,50,50,100,100,0,0,100,100);
		//context.drawImage(image, topLeft[1], topLeft[0],100,100,0,0,100,100);
	  
	  
	  
    }
  }
}


$( document ).ready(function() {
    main();
});

</script>

</body>

</html>
